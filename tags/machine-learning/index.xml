<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>machine learning on rprtr258</title><link>https://rprtr258.github.io/tags/machine-learning/</link><description>Recent content in machine learning on rprtr258</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 13 Dec 2019 21:45:56 +0000</lastBuildDate><atom:link href="https://rprtr258.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Abobus</title><link>https://rprtr258.github.io/posts/2019-12-13-21-45-56/</link><pubDate>Fri, 13 Dec 2019 21:45:56 +0000</pubDate><guid>https://rprtr258.github.io/posts/2019-12-13-21-45-56/</guid><description>Оказывается KMeans на картинка можно легко завиузализировать, показав центроиды. Пример для MNIST с цифрами 8x8 пикселей(1) Продвинутый KMeans под название GaussianMixture делает вместо круглых кластеров(потому что можно считать, что KMeans дает кластеры с центром в центроидах(вау) и радиусом до самого дальнего обьекта кластера) эллипсоидные кластеры. И, хотя, вычислительно этот алгоритм такой себе, он тем не менее дают простую генеративную модель: из этого распределения можно сэмплировать! Пример сэмплированных картинок из того же MNIST(2) Причем для уменьшения вычислений юзается PCA с сохранением 99% дисперсии, и после сэмплирования результаты преобразуются обратно.</description></item><item><title>Abobus</title><link>https://rprtr258.github.io/posts/2019-12-09-21-32-55/</link><pubDate>Mon, 09 Dec 2019 21:32:55 +0000</pubDate><guid>https://rprtr258.github.io/posts/2019-12-09-21-32-55/</guid><description>На функане препод таки сформулировал одну из теорем в виде того, что определенная &amp;ldquo;диаграмма коммутирует&amp;rdquo;. Словил каеф
Сожрали с ним пиццу, заболели животы(???)
Пытался разобраться в проекте одногруппника, где он хотел застекать
RPCA -&amp;gt; CNN -&amp;gt; Lasso -&amp;gt; ANN Classifier
Пытались разобраться, что такое RPCA(что такое PCA я и так узнал недавно из курса, который, надеюсь, когда-нибудь закончится) и какой смысл в SVD. Оказывается SVD(сингулярное разложение матрицы) очень крутая штука, но надо в ней разобраться, чтобы понять ее смысл.</description></item></channel></rss>